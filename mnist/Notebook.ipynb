{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "def plot_image(image):\n",
    "    plt.imshow(image.reshape((28,28)), cmap=\"gray\")\n",
    "    \n",
    "features = get_features(\"train_features.gz\").reshape((60000, 784))\n",
    "features = features / 255\n",
    "features = features.astype(np.float32)\n",
    "test = get_features(\"test_features.gz\").reshape((10000, 784))\n",
    "test = test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(0 - x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.where(x > 0, np.full(np.shape(x), 1), np.full(np.shape(x), 0))\n",
    "\n",
    "def squared_loss(y, x):\n",
    "    return (y - x) ** 2 / 2\n",
    "\n",
    "def d_squared_loss(y, x):\n",
    "    return - (y - x)\n",
    "\n",
    "def cross_entropy_loss(x,y):\n",
    "    e = 0.0000001\n",
    "    return -x*np.log(y+e) - (1-x)*np.log(e+1-y)\n",
    "\n",
    "def d_cross_entropy_loss(x,y):\n",
    "    return ((1-x)/(1-y)) - (x/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(xs, w_1, w_2, b_1, b_2, no_training=False):\n",
    "    \"\"\" Performs automatic differntiation for the required network.\n",
    "    \n",
    "    :param xs: the matrix of features\n",
    "    :param w1, w2, b1, b2: the matrices and vectors for the pairs of weights and biases\n",
    "    :return:\n",
    "        the matrix representing the loss for each value in xs\n",
    "        the matrices representing the total derivative of the error function for each\n",
    "        value in xs for w1, w2, b1 and b2\n",
    "        the matrix representing the error for each value in xs \"\"\"\n",
    "    N = xs.shape[1]\n",
    "    D = xs.shape[0]\n",
    "    f1 = w_1 @ xs\n",
    "    f2 = (b_1 + f1.T).T\n",
    "    f3 = relu(f2)\n",
    "    f4 = w_2 @ f3\n",
    "    f5 = (b_2 + f4.T).T\n",
    "    f6 = sigmoid(f5)\n",
    "    f7 = squared_loss(xs, f6)\n",
    "    f8 = np.sum(f7, axis=0)\n",
    "    f9 = np.sum(f8)\n",
    "    f10 = f9 / N\n",
    "    if no_training:\n",
    "        return (f6, f10)\n",
    "    t10 = 1\n",
    "    t9 = 1 / N\n",
    "    t8 = np.full((N), 1 / N)\n",
    "    t7 = np.full((D, N), 1 / N)\n",
    "    t6 = d_squared_loss(xs, f6) * t7\n",
    "    t5 = (f6*(1-f6)) * t6\n",
    "    t4 = t5\n",
    "    tb2 = np.sum(t5, axis=1)\n",
    "\n",
    "    t3 = w_2.T @ t4\n",
    "    tw2 = (f3 @ t4.T).T\n",
    "\n",
    "    t2 = d_relu(f2) * t3\n",
    "\n",
    "    t1 = t2\n",
    "    tb1 = np.sum(t2, axis=1)\n",
    "    tw1 = t1 @ xs.T\n",
    "    return f6, tw1, tw2, tb1, tb2, f10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(weight, grad, k, m_prev, v_prev, b1=0.9, b2=0.999, l=0.0000000001, rate=0.05):\n",
    "    \"\"\" Performs ADAM optimisaton.\n",
    "    \n",
    "    :param weight: the list of weights to optimise\n",
    "    :param grad: the gradient for the error function at the weights\n",
    "    :param k: the round of optimisation\n",
    "    :param m_prev, v_prev: the ADAM parameters at the previous round\n",
    "    :return: the next round of optimisation, the ADAM parameters for the round\n",
    "    \n",
    "    \"\"\"\n",
    "    m = b1 * m_prev + (1 - b1)*grad\n",
    "    v = b2 * v_prev + (1 - b2)*(grad ** 2)\n",
    "    \n",
    "    m_hat = m / (1 - (b1 ** k))\n",
    "    v_hat = v / (1 - (b2 ** k))\n",
    "    weight -= (rate / (np.sqrt(v_hat) + l))*m_hat\n",
    "    return (k+1, m, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "def create_weight(shape):\n",
    "    stdv = 1. / np.sqrt(shape[-1])\n",
    "    return np.random.uniform(-stdv, stdv, shape).astype(np.float32)\n",
    "\n",
    "w1 = create_weight((30, 784))\n",
    "w2 = create_weight((784, 30))\n",
    "b1 = create_weight((30,))\n",
    "b2 = create_weight((784,))\n",
    "\n",
    "past_epochs = []\n",
    "past_loss_values = []\n",
    "\n",
    "epochs = 100\n",
    "previous_runnings = [[1,0,0] for i in range(4)]\n",
    "weights = [w1,w2,b1,b2]\n",
    "\n",
    "for i in range(epochs):\n",
    "    res, grad_w1, grad_w2, grad_b1, grad_b2, loss = run(features.T, w1, w2, b1, b2)\n",
    "    grads = [grad_w1, grad_w2, grad_b1, grad_b2]\n",
    "    for (j, (weight, previous_running, grad)) in enumerate(zip(weights, previous_runnings, grads)):\n",
    "        k, m, v = previous_running\n",
    "        previous_runnings[j] = adam(weight, grad, k, m, v)\n",
    "    #pickle.dump(weights, open(\"weight_square.p\", \"wb\"))\n",
    "    past_epochs.append(i)\n",
    "    past_loss_values.append(loss)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slices\n",
    "\n",
    "S = 10\n",
    "w1, w2, b1, b2 = pickle.load(open(\"weight_cross.p\", \"rb\"))\n",
    "u1 = np.random.normal(0, 1, w1.shape)\n",
    "u2 = np.random.normal(0, 1, w1.shape)\n",
    "values = np.zeros((S, S))\n",
    "\n",
    "\n",
    "for i in range(0, S):\n",
    "    z1 = i / S\n",
    "    for j in range(0, S):\n",
    "        z2 = j / S\n",
    "        _, l = run(features.T, w1 + z1 * u1 + z2 * u2, w2, b1, b2, no_training=True)\n",
    "        values[i][j] = l\n",
    "    print(\"At {} of {}.\".format(i + 1, S))        \n",
    "fig, ax = plt.subplots() \n",
    "plt.plot(values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
